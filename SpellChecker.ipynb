{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharindu021/Context_Based_Spell_Checker/blob/main/SpellChecker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKNAxKvTo1kR"
      },
      "source": [
        "import NLTK is a natural language toolkit and the wordnet is a database it can use for the spell checker and also lingustic events.and from that we can get edit distance method also"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9epBFROxo0KA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.metrics.distance import edit_distance\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiBkXpEuzelj"
      },
      "source": [
        "i try to check with in the dataset in the drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0MoTiUXzeue",
        "outputId": "25f99fc8-9eae-4b88-f346-ed7c7ba800d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# so i need to train a model using word2vec to get simillar words to the recomendation words"
      ],
      "metadata": {
        "id": "bIZNjfzAZ7Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "firstly need to get the dataset to the training the model"
      ],
      "metadata": {
        "id": "D1QIOaROl5O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Dataset/All_train_data.csv\"\n",
        "dp = pd.read_csv(dataset_path);\n",
        "dp['input'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRVYt-YXXqqG",
        "outputId": "c08a8915-40cb-4f98-d4cb-5747ba567ddb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      I am reading score of Mahler is Symphony No . .\n",
              "1    I am not interested in cars or electric applia...\n",
              "2           This is my homework for my English class .\n",
              "3    In comparison , Canada is catches increased an...\n",
              "4    Fortunately , my older sister is friend is a d...\n",
              "Name: input, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "then i need to apply preprocess before the model training"
      ],
      "metadata": {
        "id": "3SHWfeGyaSRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dp_text = dp.input.apply(gensim.utils.simple_preprocess)"
      ],
      "metadata": {
        "id": "W01sKooTaiew"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "after the preprocess we can tune the word2vec for our needed"
      ],
      "metadata": {
        "id": "dBynJhn8YZIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(\n",
        "    vector_size = 150,\n",
        "    window = 10,\n",
        "    workers = 6,\n",
        "    min_count = 4\n",
        ")"
      ],
      "metadata": {
        "id": "cUfmEOLsVDZD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then we can train the model using the dataset the we can finalize the trainning"
      ],
      "metadata": {
        "id": "ZVUvV3-bpoPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.build_vocab(dp_text,progress_per=100)\n",
        "model.train(dp_text, total_examples=model.corpus_count , epochs = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE6I_Po4pog_",
        "outputId": "7b54f6c5-40a6-4fb5-e174-f5be9c2ce476"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.keyedvectors:sorting after vectors have been allocated is expensive & error-prone\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(998274, 1410485)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SYe1qpiFpo2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"apple\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ0edWzzppf5",
        "outputId": "535542e8-cfe5-419b-835a-f12f1b23c10e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dvd', 0.9918265342712402),\n",
              " ('green', 0.9915356040000916),\n",
              " ('hit', 0.9914762377738953),\n",
              " ('bridge', 0.9906883239746094),\n",
              " ('camp', 0.9901688098907471),\n",
              " ('olympics', 0.9890050888061523),\n",
              " ('upon', 0.9885343313217163),\n",
              " ('waist', 0.9882704615592957),\n",
              " ('factory', 0.988155722618103),\n",
              " ('worst', 0.9867719411849976)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLjipscl39W4"
      },
      "source": [
        "in this we can retrive the data from the dataset to use to the spell checking and then finally returns as the array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cthg47kD39iq",
        "outputId": "cf9ea8b1-bcf6-404e-ae68-b70056ee2c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-7a37bfac810e>:3: DtypeWarning: Columns (3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188131    Yupon\n",
            "188132      Yux\n",
            "188133     Yvel\n",
            "188134     Ywar\n",
            "188135     Ywis\n",
            "Name: word, dtype: object\n"
          ]
        }
      ],
      "source": [
        "path = \"/content/drive/MyDrive/Dataset/english Dictionary only words.csv\"\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "dictionary = set(df['word'].str.lower())\n",
        "print(df['word'].tail())\n",
        "#with open(path,'r') as file:\n",
        "#    words = file.read().split()\n",
        "#  return set(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q54mUS6xpMp5"
      },
      "source": [
        "punkt is a tokenizer and we need to download that and the wordnet database also then we can preprocessing our project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rPFjjf9pNBU",
        "outputId": "cb7a1c08-3746-445d-dc1d-79e4d0a46ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "#nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am using downloaded pretained model useto the checking the spell checking"
      ],
      "metadata": {
        "id": "5aKYfhj_dCmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(embedding1, embedding2):\n",
        "    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
        "\n",
        "# Function to find similar words to a given sentence using Word2Vec embeddings\n",
        "def find_most_similar_word(sentence, input_words, word2vec_model, top_n=5):\n",
        "    # Tokenize sentence and filter out words not in vocabulary\n",
        "    words = [word for word in sentence.split() if word in word2vec_model.wv]\n",
        "    if not words:\n",
        "        return None  # Return None if no words are found in the vocabulary\n",
        "\n",
        "    # Compute average embedding of words in the sentence\n",
        "    sentence_embedding = np.mean([word2vec_model.wv[word] for word in words], axis=0)\n",
        "\n",
        "    # Compute similarity between sentence embedding and embeddings of input words\n",
        "    similarities = [(input_word, compute_similarity(sentence_embedding, word2vec_model.wv[input_word]))\n",
        "                    for input_word in input_words if input_word in word2vec_model.wv]\n",
        "\n",
        "    # Sort words by similarity in descending order\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return top n similar words\n",
        "    return similarities[:top_n]"
      ],
      "metadata": {
        "id": "fsAiLILtdBQ9"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHgjXrUkpuGs"
      },
      "source": [
        "from below code we can get the input sentence and that sentence can tokenize using nltk.word_tokenize then we need to chech that words one by one then if it is not in the dataset then we can print there is no word like user input and show then to the what word are the mispelled and the recomendations using edit distance algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "LWMGDs_4pubX"
      },
      "outputs": [],
      "source": [
        "def spell_checker(sentence, dictionary):\n",
        "      simillar_word = []\n",
        "      words = nltk.word_tokenize(sentence)\n",
        "      for word in words:\n",
        "          if word.lower() not in dictionary:\n",
        "             simillar_word = recomend_simillar_words(word.lower())\n",
        "             #print(word,simillar_word,sentence)\n",
        "             recomended_words = find_most_similar_word(sentence,simillar_word,model)\n",
        "             print(\"Recommended words for '{}' are: {}\".format(word, recomended_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6FnYeiGrdoF"
      },
      "source": [
        "in this code we can get the simmilar words for misspelled words that are input have i choose only below 2 distance. and then recomend to the client only 10 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LXmyy12YzUM4"
      },
      "outputs": [],
      "source": [
        "def recomend_simillar_words(words):\n",
        "  simillar_words = []\n",
        "  for dict_words in dictionary:\n",
        "    distance = edit_distance( dict_words , words )\n",
        "    if distance <= 1:\n",
        "      simillar_words.append( (dict_words , distance) )\n",
        "  simillar_words.sort(key=lambda x : x[1])\n",
        "  return [ word[0] for word in simillar_words[:20]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUcEAUzoE-3j"
      },
      "source": [
        "in that code part we can input the sentence then it strictly move to the spell_checker code part then it can check whether the word is in the dataset or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "GPNYxc5_rd5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b878cbf5-b2ce-4721-9e39-9624c86d6987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the sentence you want or type 'exit' to quit: i eat aple\n",
            "Recommended words for 'aple' are: [('apple', 0.70152116), ('able', 0.6632011)]\n",
            "Please enter the sentence you want or type 'exit' to quit: exit\n",
            "Exiting...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "user_input = \"\"\n",
        "while user_input.lower() != \"exit\":\n",
        "    user_input = input(\"Please enter the sentence you want or type 'exit' to quit: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Exiting...\")\n",
        "    else:\n",
        "        spell_checker(user_input, dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "BOv6NWW8Y1u5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bswbe6t7gGg5q82Oewtwt9dnp6TNylZU",
      "authorship_tag": "ABX9TyPJ/a0MtbgipLVqktSIkbuB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}