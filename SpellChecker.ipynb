{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharindu021/Context_Based_Spell_Checker/blob/main/SpellChecker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKNAxKvTo1kR"
      },
      "source": [
        "import NLTK is a natural language toolkit and the wordnet is a database it can use for the spell checker and also lingustic events.and from that we can get edit distance method also"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9epBFROxo0KA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.metrics.distance import edit_distance\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiBkXpEuzelj"
      },
      "source": [
        "i try to check with in the dataset in the drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0MoTiUXzeue",
        "outputId": "db1120e9-6a2e-401f-9815-5710a4973674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# so i need to train a model using word2vec to get simillar words to the recomendation words"
      ],
      "metadata": {
        "id": "bIZNjfzAZ7Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "firstly need to get the dataset to the training the model"
      ],
      "metadata": {
        "id": "D1QIOaROl5O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset_path = \"/content/drive/MyDrive/Dataset/All_train_data.csv\"\n",
        "#dp = pd.read_csv(dataset_path);\n",
        "#dp['input'].head()\n",
        "from gensim.models import KeyedVectors\n",
        "model_path = \"/content/drive/MyDrive/Dataset/Models/google_news.kv\"\n",
        "model = KeyedVectors.load(model_path)\n"
      ],
      "metadata": {
        "id": "WRVYt-YXXqqG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then i need to apply preprocess before the model training"
      ],
      "metadata": {
        "id": "3SHWfeGyaSRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dp_text = dp.input.apply(gensim.utils.simple_preprocess)\n",
        "from datasets import list_datasets\n",
        "\n",
        "# Get the list of available datasets\n",
        "datasets_list = list_datasets()\n",
        "\n",
        "# Print the list of datasets\n",
        "for dataset_name in datasets_list:\n",
        "    print(dataset_name)\n"
      ],
      "metadata": {
        "id": "W01sKooTaiew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "after the preprocess we can tune the word2vec for our needed"
      ],
      "metadata": {
        "id": "dBynJhn8YZIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(\n",
        "    vector_size = 150,\n",
        "    window = 10,\n",
        "    workers = 6,\n",
        "    min_count = 4\n",
        ")"
      ],
      "metadata": {
        "id": "cUfmEOLsVDZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then we can train the model using the dataset the we can finalize the trainning"
      ],
      "metadata": {
        "id": "ZVUvV3-bpoPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.build_vocab(dp_text,progress_per=100)\n",
        "model.train(dp_text, total_examples=model.corpus_count , epochs = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE6I_Po4pog_",
        "outputId": "7b54f6c5-40a6-4fb5-e174-f5be9c2ce476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.keyedvectors:sorting after vectors have been allocated is expensive & error-prone\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(998274, 1410485)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SYe1qpiFpo2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model[\"apple\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ0edWzzppf5",
        "outputId": "a29041d9-55c9-4a9a-8ca4-61953e0ff012"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.06445312, -0.16015625, -0.01208496,  0.13476562, -0.22949219,\n",
              "        0.16210938,  0.3046875 , -0.1796875 , -0.12109375,  0.25390625,\n",
              "       -0.01428223, -0.06396484, -0.08056641, -0.05688477, -0.19628906,\n",
              "        0.2890625 , -0.05151367,  0.14257812, -0.10498047, -0.04736328,\n",
              "       -0.34765625,  0.35742188,  0.265625  ,  0.00188446, -0.01586914,\n",
              "        0.00195312, -0.35546875,  0.22167969,  0.05761719,  0.15917969,\n",
              "        0.08691406, -0.0267334 , -0.04785156,  0.23925781, -0.05981445,\n",
              "        0.0378418 ,  0.17382812, -0.41796875,  0.2890625 ,  0.32617188,\n",
              "        0.02429199, -0.01647949, -0.06494141, -0.08886719,  0.07666016,\n",
              "       -0.15136719,  0.05249023, -0.04199219, -0.05419922,  0.00108337,\n",
              "       -0.20117188,  0.12304688,  0.09228516,  0.10449219, -0.00408936,\n",
              "       -0.04199219,  0.01409912, -0.02111816, -0.13476562, -0.24316406,\n",
              "        0.16015625, -0.06689453, -0.08984375, -0.07177734, -0.00595093,\n",
              "       -0.00482178, -0.00089264, -0.30664062, -0.0625    ,  0.07958984,\n",
              "       -0.00909424, -0.04492188,  0.09960938, -0.33398438, -0.3984375 ,\n",
              "        0.05541992, -0.06689453, -0.04467773,  0.11767578, -0.13964844,\n",
              "       -0.26367188,  0.17480469, -0.17382812, -0.40625   , -0.06738281,\n",
              "       -0.07617188,  0.09423828,  0.20996094, -0.16308594, -0.08691406,\n",
              "       -0.0534668 , -0.10351562, -0.07617188, -0.11083984, -0.03515625,\n",
              "       -0.14941406,  0.0378418 ,  0.38671875,  0.14160156, -0.2890625 ,\n",
              "       -0.16894531, -0.140625  , -0.04174805,  0.22753906,  0.24023438,\n",
              "       -0.01599121, -0.06787109,  0.21875   , -0.42382812, -0.5625    ,\n",
              "       -0.49414062, -0.3359375 ,  0.13378906,  0.01141357,  0.13671875,\n",
              "        0.0324707 ,  0.06835938, -0.27539062, -0.15917969,  0.00121307,\n",
              "        0.01208496, -0.0039978 ,  0.00442505, -0.04541016,  0.08642578,\n",
              "        0.09960938, -0.04296875, -0.11328125,  0.13867188,  0.41796875,\n",
              "       -0.28320312, -0.07373047, -0.11425781,  0.08691406, -0.02148438,\n",
              "        0.328125  , -0.07373047, -0.01348877,  0.17773438, -0.02624512,\n",
              "        0.13378906, -0.11132812, -0.12792969, -0.12792969,  0.18945312,\n",
              "       -0.13867188,  0.29882812, -0.07714844, -0.37695312, -0.10351562,\n",
              "        0.16992188, -0.10742188, -0.29882812,  0.00866699, -0.27734375,\n",
              "       -0.20996094, -0.1796875 , -0.19628906, -0.22167969,  0.08886719,\n",
              "       -0.27734375, -0.13964844,  0.15917969,  0.03637695,  0.03320312,\n",
              "       -0.08105469,  0.25390625, -0.08691406, -0.21289062, -0.18945312,\n",
              "       -0.22363281,  0.06542969, -0.16601562,  0.08837891, -0.359375  ,\n",
              "       -0.09863281,  0.35546875, -0.00741577,  0.19042969,  0.16992188,\n",
              "       -0.06005859, -0.20605469,  0.08105469,  0.12988281, -0.01135254,\n",
              "        0.33203125, -0.08691406,  0.27539062, -0.03271484,  0.12011719,\n",
              "       -0.0625    ,  0.1953125 , -0.10986328, -0.11767578,  0.20996094,\n",
              "        0.19921875,  0.02954102, -0.16015625,  0.00276184, -0.01367188,\n",
              "        0.03442383, -0.19335938,  0.00352478, -0.06542969, -0.05566406,\n",
              "        0.09423828,  0.29296875,  0.04052734, -0.09326172, -0.10107422,\n",
              "       -0.27539062,  0.04394531, -0.07275391,  0.13867188,  0.02380371,\n",
              "        0.13085938,  0.00236511, -0.2265625 ,  0.34765625,  0.13574219,\n",
              "        0.05224609,  0.18164062,  0.0402832 ,  0.23730469, -0.16992188,\n",
              "        0.10058594,  0.03833008,  0.10839844, -0.05615234, -0.00946045,\n",
              "        0.14550781, -0.30078125, -0.32226562,  0.18847656, -0.40234375,\n",
              "       -0.3125    , -0.08007812, -0.26757812,  0.16699219,  0.07324219,\n",
              "        0.06347656,  0.06591797,  0.17285156, -0.17773438,  0.00276184,\n",
              "       -0.05761719, -0.2265625 , -0.19628906,  0.09667969,  0.13769531,\n",
              "       -0.49414062, -0.27929688,  0.12304688, -0.30078125,  0.01293945,\n",
              "       -0.1875    , -0.20898438, -0.1796875 , -0.16015625, -0.03295898,\n",
              "        0.00976562,  0.25390625, -0.25195312,  0.00210571,  0.04296875,\n",
              "        0.01184082, -0.20605469,  0.24804688, -0.203125  , -0.17773438,\n",
              "        0.07275391,  0.04541016,  0.21679688, -0.2109375 ,  0.14550781,\n",
              "       -0.16210938,  0.20410156, -0.19628906, -0.35742188,  0.35742188,\n",
              "       -0.11962891,  0.35742188,  0.10351562,  0.07080078, -0.24707031,\n",
              "       -0.10449219, -0.19238281,  0.1484375 ,  0.00057983,  0.296875  ,\n",
              "       -0.12695312, -0.03979492,  0.13183594, -0.16601562,  0.125     ,\n",
              "        0.05126953, -0.14941406,  0.13671875, -0.02075195,  0.34375   ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLjipscl39W4"
      },
      "source": [
        "in this we can retrive the data from the dataset to use to the spell checking and then finally returns as the array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cthg47kD39iq",
        "outputId": "976010d1-b61a-4791-8d3b-b83097c494ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-7a37bfac810e>:3: DtypeWarning: Columns (3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188131    Yupon\n",
            "188132      Yux\n",
            "188133     Yvel\n",
            "188134     Ywar\n",
            "188135     Ywis\n",
            "Name: word, dtype: object\n"
          ]
        }
      ],
      "source": [
        "path = \"/content/drive/MyDrive/Dataset/english Dictionary only words.csv\"\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "dictionary = set(df['word'].str.lower())\n",
        "print(df['word'].tail())\n",
        "#with open(path,'r') as file:\n",
        "#    words = file.read().split()\n",
        "#  return set(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q54mUS6xpMp5"
      },
      "source": [
        "punkt is a tokenizer and we need to download that and the wordnet database also then we can preprocessing our project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rPFjjf9pNBU",
        "outputId": "16b5ebce-f564-40bd-f0eb-fc89765936e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "#nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " i use cosine similarity to measure the similarity between two vectors\n"
      ],
      "metadata": {
        "id": "UG_BrnVn1-AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(vector1, vector2):\n",
        "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
      ],
      "metadata": {
        "id": "a008V7ic19CZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am using downloaded pretained model useto the checking the spell checking and also following function is to find most similar_word in given input words to the sentence, Firstly it tokenise the sentence and then check the words are in the model if not it return none and next get the sentence mean base on the word it store in the sentence_mean then it measure the cosine similarity between sentence and relevant word and it label to the word then its store in the simillarities variable then sort it in descending order return the most 5 similar word."
      ],
      "metadata": {
        "id": "5aKYfhj_dCmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_similar_word(sentence, input_words, word2vec_model):\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "    sentence = nltk.word_tokenize(sentence)\n",
        "    words = [word for word in sentence if word in word2vec_model]\n",
        "    if not words:\n",
        "        return None\n",
        "\n",
        "    sentence_mean = np.mean([word2vec_model[word] for word in words], axis=0)\n",
        "    input_words = [input_word for input_word in input_words if input_word in word2vec_model]\n",
        "    similarities = [(label, compute_similarity(sentence_mean, word2vec_model[label])) for label in input_words]\n",
        "\n",
        "    print(similarities)\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "    return similarities[0:5]"
      ],
      "metadata": {
        "id": "fsAiLILtdBQ9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHgjXrUkpuGs"
      },
      "source": [
        "from below code we can get the input sentence and that sentence can tokenize using nltk.word_tokenize then we need to chech that words one by one then if it is not in the dataset then we can print there is no word like user input and show then to the what word are the mispelled and the recomendations using edit distance algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LWMGDs_4pubX"
      },
      "outputs": [],
      "source": [
        "def spell_checker(sentence, dictionary):\n",
        "      simillar_word = []\n",
        "      sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "      words = nltk.word_tokenize(sentence)\n",
        "      for word in words:\n",
        "          if word.lower() not in dictionary:\n",
        "             simillar_word = recomend_simillar_words(word.lower())\n",
        "             #print(word,simillar_word,sentence)\n",
        "             recomended_words = find_most_similar_word(sentence,simillar_word,model)\n",
        "             print(\"Recommended words for '{}' are: {}\".format(word, recomended_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6FnYeiGrdoF"
      },
      "source": [
        "in this code we can get the simmilar words for misspelled words that are input have i choose only below 2 distance. and then recomend to the client only 10 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LXmyy12YzUM4"
      },
      "outputs": [],
      "source": [
        "def recomend_simillar_words(words):\n",
        "  simillar_words = []\n",
        "  for dict_words in dictionary:\n",
        "    distance = edit_distance( dict_words , words )\n",
        "    if distance <= 1:\n",
        "      simillar_words.append( (dict_words , distance) )\n",
        "  simillar_words.sort(key=lambda x : x[1])\n",
        "  return [ word[0] for word in simillar_words[:20]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUcEAUzoE-3j"
      },
      "source": [
        "in that code part we can input the sentence then it strictly move to the spell_checker code part then it can check whether the word is in the dataset or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "GPNYxc5_rd5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd447ab-eb6b-4d5d-8c83-e19fb566e3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the sentence you want or type 'exit' to quit: i eat aple\n",
            "[('axle', 0.08812634), ('able', 0.18671234), ('maple', 0.13715714), ('apple', 0.3208351), ('ale', 0.17726763), ('apse', 0.058557026), ('ample', 0.079364814), ('ape', 0.19740877)]\n",
            "Recommended words for 'aple' are: [('apple', 0.3208351), ('ape', 0.19740877), ('able', 0.18671234), ('ale', 0.17726763), ('maple', 0.13715714)]\n",
            "Please enter the sentence you want or type 'exit' to quit: i eat aple.\n",
            "[('axle', 0.08812634), ('able', 0.18671234), ('maple', 0.13715714), ('apple', 0.3208351), ('ale', 0.17726763), ('apse', 0.058557026), ('ample', 0.079364814), ('ape', 0.19740877)]\n",
            "Recommended words for 'aple' are: [('apple', 0.3208351), ('ape', 0.19740877), ('able', 0.18671234), ('ale', 0.17726763), ('maple', 0.13715714)]\n",
            "Please enter the sentence you want or type 'exit' to quit: exit\n",
            "Exiting...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "user_input = \"\"\n",
        "while user_input.lower() != \"exit\":\n",
        "    user_input = input(\"Please enter the sentence you want or type 'exit' to quit: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Exiting...\")\n",
        "    else:\n",
        "        spell_checker(user_input, dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOv6NWW8Y1u5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBfrFri4rgj_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1bswbe6t7gGg5q82Oewtwt9dnp6TNylZU",
      "authorship_tag": "ABX9TyPZ/IEHj6Nh92tDuXKgrd8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}